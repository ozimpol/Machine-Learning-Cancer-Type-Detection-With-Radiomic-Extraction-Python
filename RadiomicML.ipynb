{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1472f2",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center; font-size:64px;\">Data Equalization</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09ec71",
   "metadata": {},
   "source": [
    "<p>This part provide to duplicate the malignant datas. At first, There were 437 benign datas and 210 malignant datas.</p>\n",
    "<p>We need to double malignant datas to balance dataset.</p>\n",
    "<p>Using malignant datas' reflection on axis y will solve the problem.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2142b",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e211ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199fe34",
   "metadata": {},
   "source": [
    "#### reflection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64717f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_image_y(image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "    return flipped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460000d",
   "metadata": {},
   "source": [
    "#### save ultrasound images method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ba2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_flipped_images(folder_path, output_folder, prefix):\n",
    "    \n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\") and prefix in file and \"_mask\" not in file:\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            flipped_image = flip_image_y(image_path)\n",
    "            file_parts = file.split(prefix)\n",
    "            if len(file_parts) > 1:\n",
    "                index = file_parts[1].split(\".\")[0]\n",
    "                if index.isdigit():\n",
    "                    index = int(index)\n",
    "                    new_file_name = prefix + str(index + 210) + \".png\"\n",
    "                    output_path = os.path.join(output_folder, new_file_name)\n",
    "                    cv2.imwrite(output_path, flipped_image)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed36767",
   "metadata": {},
   "source": [
    "#### save marked images method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20dcf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_flipped_masks(folder_path, output_folder, prefix):\n",
    "\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith(\".png\") and prefix in file:\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            flipped_image = flip_image_y(image_path)\n",
    "            file_parts = file.split(prefix)\n",
    "            if len(file_parts) > 1:\n",
    "                index = file_parts[1].split(\"_\")[0]\n",
    "                if index.isdigit():\n",
    "                    index = int(index)\n",
    "                    new_file_name = prefix + str(index + 210) + \"_mask.png\"\n",
    "                    output_path = os.path.join(output_folder, new_file_name)\n",
    "                    cv2.imwrite(output_path, flipped_image)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bfb5fd",
   "metadata": {},
   "source": [
    "#### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"images/\"\n",
    "output_folder = \"images/\"\n",
    "\n",
    "save_flipped_images(input_folder, output_folder, \"malignant\")\n",
    "save_flipped_masks(input_folder, output_folder, \"malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071d6ef",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center; font-size:64px;\">Radiomic Feature Extraction</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245ccec",
   "metadata": {},
   "source": [
    "<p>This part provides extracting radiomic features from images.</p>\n",
    "<p>In this part, we use yaml, SimpleITK and radiomics libraries.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e577032",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ab948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "from radiomics import featureextractor\n",
    "from radiomics.featureextractor import getFeatureClasses\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99197843",
   "metadata": {},
   "source": [
    "#### load parameters and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Params.yaml\", \"r\") as stream:\n",
    "    params = yaml.safe_load(stream)\n",
    "\n",
    "image_folder = \"images/\"\n",
    "results = []\n",
    "\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de38b3b",
   "metadata": {},
   "source": [
    " #### loop for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith(\".png\") and \"_mask\" not in filename:\n",
    "        try:\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "            image = sitk.ReadImage(image_path)\n",
    "\n",
    "            image_array = sitk.GetArrayFromImage(image)\n",
    "            if image.GetNumberOfComponentsPerPixel() > 1:\n",
    "                image = sitk.VectorIndexSelectionCast(image, 0, sitk.sitkUInt8)\n",
    "            else:\n",
    "                image = sitk.Cast(image, sitk.sitkUInt8)\n",
    "\n",
    "            mask_filename = filename.split(\".\")[0] + \"_mask.png\"\n",
    "            mask_path = os.path.join(image_folder, mask_filename)\n",
    "\n",
    "            if os.path.exists(mask_path):\n",
    "                mask = sitk.ReadImage(mask_path)\n",
    "\n",
    "                features = extractor.execute(image, mask)\n",
    "                features[\"ultrasound\"] = filename\n",
    "                cancertype = \"benign\" if \"benign\" in filename else \"malignant\"\n",
    "                features[\"cancertype\"] = cancertype\n",
    "\n",
    "                results.append(features)\n",
    "            else:\n",
    "                print(f\"{mask_filename} adlı maske bulunamadı, işlem atlandı.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Hata oluştu: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca00ddb",
   "metadata": {},
   "source": [
    "#### creat csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b271cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "ultrasound_col = results_df.pop(\"ultrasound\")\n",
    "results_df.insert(0, \"ultrasound\", ultrasound_col)\n",
    "\n",
    "results_df.to_csv(\"radiomic_features.csv\", index=False)\n",
    "\n",
    "radiomic_features_df = pd.read_csv(\"radiomic_features.csv\")\n",
    "\n",
    "benign_df = radiomic_features_df[radiomic_features_df[\"cancertype\"] == \"benign\"]\n",
    "malignant_df = radiomic_features_df[radiomic_features_df[\"cancertype\"] == \"malignant\"]\n",
    "\n",
    "benign_test = benign_df.tail(50)\n",
    "malignant_test = malignant_df.tail(50)\n",
    "test_df = pd.concat([benign_test, malignant_test])\n",
    "\n",
    "benign_train = benign_df.iloc[:-50]\n",
    "malignant_train = malignant_df.iloc[:-50]\n",
    "train_df = pd.concat([benign_train, malignant_train])\n",
    "\n",
    "test_df.to_csv(\"radiomic_test.csv\", index=False)\n",
    "train_df.to_csv(\"radiomic_train.csv\", index=False)\n",
    "\n",
    "print(\"Radiomic Features DataFrame:\")\n",
    "print(radiomic_features_df.head())\n",
    "\n",
    "print(\"\\nTrain DataFrame:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21656452",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center; font-size:64px;\">Feature Selection</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc78f4",
   "metadata": {},
   "source": [
    "<p>This part provide to calculate feature importance scores for each model and each feature.</p>\n",
    "<p>Feature importance score and correlation values help us to choose features that we use.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056bf321",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "from openpyxl.formatting.rule import CellIsRule\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "from openpyxl import load_workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fb2bc",
   "metadata": {},
   "source": [
    "#### define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee225e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"radiomic_features.csv\")\n",
    "\n",
    "X = results_df[[\"diagnostics_Versions_PyRadiomics\",\"diagnostics_Versions_Numpy\",\"diagnostics_Versions_SimpleITK\",\n",
    "\"diagnostics_Versions_PyWavelet\",\"diagnostics_Versions_Python\",\"diagnostics_Configuration_Settings\",\n",
    "\"diagnostics_Configuration_EnabledImageTypes\",\"original_shape2D_MeshSurface\",\"original_shape2D_PixelSurface\",\n",
    "\"original_shape2D_Perimeter\",\"original_shape2D_PerimeterSurfaceRatio\",\"original_shape2D_Sphericity\",\n",
    "\"original_shape2D_MaximumDiameter\",\"original_shape2D_MajorAxisLength\",\"original_shape2D_MinorAxisLength\",\n",
    "\"original_shape2D_Elongation\",\"original_firstorder_Energy\",\"original_firstorder_TotalEnergy\",\"original_firstorder_10Percentile\",\n",
    "\"original_firstorder_90Percentile\",\"original_firstorder_Mean\",\"original_firstorder_Variance\",\"original_firstorder_Skewness\",\n",
    "\"original_firstorder_Kurtosis\",\"original_firstorder_Maximum\",\"original_firstorder_Minimum\",\"original_firstorder_Median\",\n",
    "\"original_firstorder_InterquartileRange\",\"original_firstorder_Range\",\"original_firstorder_MeanAbsoluteDeviation\",\n",
    "\"original_firstorder_Entropy\",\"original_firstorder_RobustMeanAbsoluteDeviation\",\"original_firstorder_RootMeanSquared\",\n",
    "\"original_firstorder_Uniformity\",\"original_glcm_Autocorrelation\",\"original_glcm_JointAverage\",\"original_glcm_ClusterProminence\",\n",
    "\"original_glcm_ClusterShade\",\"original_glcm_ClusterTendency\",\"original_glcm_Contrast\",\"original_glcm_Correlation\",\n",
    "\"original_glcm_DifferenceAverage\",\"original_glcm_DifferenceEntropy\",\"original_glcm_DifferenceVariance\",\n",
    "\"original_glcm_JointEnergy\",\"original_glcm_JointEntropy\",\"original_glcm_Imc1\",\"original_glcm_Imc2\",\"original_glcm_Idm\",\n",
    "\"original_glcm_Idmn\",\"original_glcm_Id\",\"original_glcm_Idn\",\"original_glcm_InverseVariance\",\"original_glcm_MaximumProbability\",\n",
    "\"original_glcm_SumEntropy\",\"original_glcm_SumSquares\",\"original_glrlm_ShortRunEmphasis\",\"original_glrlm_LongRunEmphasis\",\n",
    "\"original_glrlm_GrayLevelNonUniformity\",\"original_glrlm_GrayLevelNonUniformityNormalized\",\"original_glrlm_RunLengthNonUniformity\",\n",
    "\"original_glrlm_RunLengthNonUniformityNormalized\",\"original_glrlm_RunPercentage\",\"original_glrlm_GrayLevelVariance\",\n",
    "\"original_glrlm_RunVariance\",\"original_glrlm_RunEntropy\",\"original_glrlm_LowGrayLevelRunEmphasis\",\n",
    "\"original_glrlm_HighGrayLevelRunEmphasis\",\"original_glrlm_ShortRunLowGrayLevelEmphasis\",\n",
    "\"original_glrlm_ShortRunHighGrayLevelEmphasis\",\"original_glrlm_LongRunLowGrayLevelEmphasis\",\n",
    "\"original_glrlm_LongRunHighGrayLevelEmphasis\",\"original_glszm_SmallAreaEmphasis\",\"original_glszm_LargeAreaEmphasis\",\n",
    "\"original_glszm_GrayLevelNonUniformity\",\"original_glszm_SizeZoneNonUniformity\",\"original_glszm_SizeZoneNonUniformityNormalized\",\n",
    "\"original_glszm_ZonePercentage\",\"original_glszm_GrayLevelVariance\",\"original_glszm_ZoneVariance\",\"original_glszm_ZoneEntropy\",\n",
    "\"original_glszm_LowGrayLevelZoneEmphasis\",\"original_glszm_HighGrayLevelZoneEmphasis\",\n",
    "\"original_glszm_SmallAreaLowGrayLevelEmphasis\",\"original_glszm_SmallAreaHighGrayLevelEmphasis\",\n",
    "\"original_glszm_LargeAreaLowGrayLevelEmphasis\",\"original_glszm_LargeAreaHighGrayLevelEmphasis\",\n",
    "\"original_gldm_GrayLevelNonUniformity\",\"original_gldm_SmallDependenceEmphasis\",\"original_gldm_LargeDependenceEmphasis\",\n",
    "\"original_gldm_DependenceNonUniformityNormalized\",\"original_gldm_GrayLevelVariance\",\"original_gldm_DependenceNonUniformity\",\n",
    "\"original_gldm_DependenceEntropy\",\"original_gldm_DependenceVariance\",\"original_gldm_LowGrayLevelEmphasis\",\n",
    "\"original_gldm_HighGrayLevelEmphasis\",\"original_gldm_SmallDependenceLowGrayLevelEmphasis\",\n",
    "\"original_gldm_SmallDependenceHighGrayLevelEmphasis\",\"original_gldm_LargeDependenceLowGrayLevelEmphasis\",\n",
    "\"original_gldm_LargeDependenceHighGrayLevelEmphasis\",\"original_ngtdm_Coarseness\",\"original_ngtdm_Contrast\",\n",
    "\"original_ngtdm_Busyness\",\"original_ngtdm_Complexity\",\"original_ngtdm_Strength\"]]\n",
    "\n",
    "y = results_df[\"cancertype\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01d5ed",
   "metadata": {},
   "source": [
    "#### calculate and save feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535656ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "(\"num\", StandardScaler(), X.select_dtypes(include=['float64', 'int64']).columns),\n",
    "(\"cat\", OneHotEncoder(handle_unknown='ignore'), X.select_dtypes(include=['object']).columns)\n",
    "])\n",
    "\n",
    "classifiers = {\n",
    "\"Random Forest\": RandomForestClassifier(),\n",
    "\"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "\"XGBoost\": XGBClassifier(),\n",
    "\"SVM\": SVC(kernel='linear'),\n",
    "\"AdaBoost\": AdaBoostClassifier(estimator=DecisionTreeClassifier(), learning_rate=0.1),\n",
    "\"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "feature_importance_dfs = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Calculating feature importance scores for {name}...\")\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", clf)])\n",
    "feature_names = preprocessor.fit(X).get_feature_names_out()\n",
    "pipeline.fit(X, y_encoded)\n",
    "\n",
    "if hasattr(clf, \"feature_importances_\"):\n",
    "    importances = clf.feature_importances_\n",
    "elif hasattr(clf, \"coef_\"):\n",
    "    importances = clf.coef_[0]\n",
    "else:\n",
    "    importances = None\n",
    "if importances is not None:\n",
    "    feature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "    feature_importance_dfs[name] = feature_importance_df\n",
    "\n",
    "for name, df in feature_importance_dfs.items():\n",
    "    df_sorted = df.sort_values(by=\"Importance\", ascending=False)\n",
    "    df_sorted.to_excel(f\"documents/{name}_feature_importance_scores.xlsx\", index=False)\n",
    "    print(f\"Feature importance scores for {name} saved to {name}_feature_importance_scores.xlsx\")\n",
    "    print(f\"\\nFeature importance scores for {name}:\")\n",
    "    df_preview = pd.read_excel(f\"documents/{name}_feature_importance_scores.xlsx\")\n",
    "    print(df_preview.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fdd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ad8b45a",
   "metadata": {},
   "source": [
    "#### calculate and save correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e264198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_features = X.select_dtypes(include=['float64', 'int64'])\n",
    "correlation_matrix = numeric_features.corr()\n",
    "\n",
    "correlation_matrix.to_excel(\"documents/feature_correlation_matrix.xlsx\")\n",
    "print(\"Feature correlation matrix saved to feature_correlation_matrix.xlsx\")\n",
    "\n",
    "file_path = \"documents/feature_correlation_matrix.xlsx\"\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    "ws = wb.active\n",
    "\n",
    "for row in ws.iter_rows():\n",
    "    for cell in row:\n",
    "        value = cell.value\n",
    "        if isinstance(value, (int, float)) and abs(value) > 0.60:\n",
    "            cell.fill = openpyxl.styles.PatternFill(start_color=\"c6efce\", end_color=\"c6efce\", fill_type=\"solid\")\n",
    "\n",
    "wb.save(file_path)\n",
    "\n",
    "correlation_matrix_preview = pd.read_excel(\"documents/feature_correlation_matrix.xlsx\")\n",
    "print(\"\\nFeature correlation matrix:\")\n",
    "print(correlation_matrix_preview.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffe74d",
   "metadata": {},
   "source": [
    "#### Automatic Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ffb18",
   "metadata": {},
   "source": [
    "In this section, we create a composite score with mean importance and correlation indexes. And we are going to use best 30 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c970a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), X.select_dtypes(include=['float64', 'int64']).columns),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), X.select_dtypes(include=['object']).columns)\n",
    "    ])\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"SVM\": SVC(kernel='linear'),\n",
    "    \"AdaBoost\": AdaBoostClassifier(estimator=DecisionTreeClassifier(), learning_rate=0.1),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "feature_importances = pd.DataFrame(index=X.columns)\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Calculating feature importance scores for {name}...\")\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", clf)])\n",
    "    pipeline.fit(X, y_encoded)\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        importances = clf.feature_importances_\n",
    "    elif hasattr(clf, \"coef_\"):\n",
    "        importances = np.abs(clf.coef_[0])\n",
    "    else:\n",
    "        importances = np.zeros(len(X.columns))  \n",
    "    feature_importances[name] = importances\n",
    "\n",
    "feature_importances = feature_importances.fillna(0)\n",
    "feature_importances['Mean_Importance'] = feature_importances.mean(axis=1)\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64'])\n",
    "correlation_matrix = numeric_features.corr().abs()\n",
    "mean_correlation = correlation_matrix.abs().mean(axis=1)\n",
    "composite_score = feature_importances['Mean_Importance'] * mean_correlation.reindex(feature_importances.index).fillna(0)\n",
    "top_features = composite_score.index\n",
    "\n",
    "selected_features_df = pd.DataFrame({\n",
    "    'Feature': top_features,\n",
    "    'Composite_Score': composite_score[top_features]\n",
    "}).sort_values(by='Composite_Score', ascending=False)\n",
    "\n",
    "selected_features_df.to_excel(\"selected_features_with_scores.xlsx\", index=False)\n",
    "print(f\"Selected features and their composite scores saved to selected_features_with_scores.xlsx\")\n",
    "\n",
    "output_path = \"selected_features_with_scores.xlsx\"\n",
    "selected_features_preview = pd.read_excel(output_path)\n",
    "print(\"\\nSelected features with their composite scores:\")\n",
    "print(selected_features_preview.head())\n",
    "\n",
    "X_selected = X[top_features]\n",
    "X_selected.to_excel(\"X_selected.xlsx\", index=False)\n",
    "print(f\"Selected features data saved to X_selected.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87960325",
   "metadata": {},
   "source": [
    "#### Feature Selection with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0559dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), X.select_dtypes(include=['float64', 'int64']).columns),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), X.select_dtypes(include=['object']).columns)\n",
    "    ])\n",
    "\n",
    "def conditional_formatting(value):\n",
    "    return value > 0.1\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"pca\", pca)])\n",
    "pipeline.fit(X)\n",
    "pca_components = pipeline.named_steps['pca'].components_\n",
    "\n",
    "original_features = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_components, columns=original_features)\n",
    "\n",
    "pca_df.index = [f\"PC{i+1}\" for i in range(pca_df.shape[0])]\n",
    "\n",
    "output_file = \"PCA_components.xlsx\"\n",
    "pca_df.to_excel(output_file, index=True)\n",
    "\n",
    "wb = openpyxl.load_workbook(\"PCA_components.xlsx\")\n",
    "\n",
    "sheet = wb.active\n",
    "\n",
    "for row in sheet.iter_rows(min_row=2, max_row=sheet.max_row, min_col=2, max_col=sheet.max_column):\n",
    "    for cell in row:\n",
    "        if cell.value > 0.1:\n",
    "            cell.fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")\n",
    "\n",
    "wb.save(\"PCA_components.xlsx\")\n",
    "\n",
    "print(f\"PCA bileşen matrisi {output_file} dosyasına kaydedildi.\")\n",
    "\n",
    "pca_df = pd.read_excel(\"PCA_components.xlsx\", index_col=0)\n",
    "\n",
    "wb = openpyxl.load_workbook(\"PCA_components.xlsx\")\n",
    "sheet = wb.active\n",
    "\n",
    "new_wb = openpyxl.Workbook()\n",
    "new_sheet = new_wb.active\n",
    "\n",
    "new_sheet.append([\"PCA Bileşeni\", \"Özellikler\"])\n",
    "\n",
    "for pc_name in pca_df.index:\n",
    "    feature_names = [col for col, value in pca_df.loc[pc_name].items() if conditional_formatting(value)]\n",
    "    \n",
    "    feature_string = ', '.join(feature_names)\n",
    "    feature_string = '\"' + feature_string + '\"'\n",
    "    \n",
    "    new_sheet.append([pc_name, feature_string])\n",
    "\n",
    "new_wb.save(\"PCA_component_features.xlsx\")\n",
    "\n",
    "new_wb.close()\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"SVM\": SVC(kernel='linear'),  \n",
    "    \"AdaBoost\": AdaBoostClassifier(estimator=DecisionTreeClassifier(), learning_rate=0.1), \n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "feature_importance_dfs = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Calculating feature importance scores for {name} with PCA...\")\n",
    "    \n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"pca\", pca), (\"classifier\", clf)])\n",
    "    \n",
    "    pipeline.fit(X, y_encoded)\n",
    "    \n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        importances = clf.feature_importances_\n",
    "    elif hasattr(clf, \"coef_\"):\n",
    "        importances = clf.coef_[0]\n",
    "    else:\n",
    "        importances = None\n",
    "    \n",
    "    if importances is not None:\n",
    "        feature_names = [f\"PC{i+1}\" for i in range(len(importances))]\n",
    "        feature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "        feature_importance_dfs[name] = feature_importance_df\n",
    "\n",
    "for name, df in feature_importance_dfs.items():\n",
    "    df_sorted = df.sort_values(by=\"Importance\", ascending=False)\n",
    "    df_sorted.to_excel(f\"documents/{name}_PCA_feature_importance_scores.xlsx\", index=False)\n",
    "    print(f\"Feature importance scores for {name} with PCA saved to {name}_PCA_feature_importance_scores.xlsx\")\n",
    "    print(f\"\\nFeature importance scores for {name} with PCA:\")\n",
    "    df_preview = pd.read_excel(f\"documents/{name}_PCA_feature_importance_scores.xlsx\")\n",
    "    print(df_preview.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce164b",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center; font-size:64px;\">Training Models with All Features</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97933345",
   "metadata": {},
   "source": [
    "<p> In this part, we train the models with best parameters and save them.</p>\n",
    "<p> We use Random Forest, Support Vector Machine, XGBoost, Gradient Boosting, AdaBoost, Decision Tree.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0b866",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68017830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f7686",
   "metadata": {},
   "source": [
    "#### load dataset and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "results_df = pd.read_csv(\"radiomic_train.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = results_df.drop(columns=[\"cancertype\", \"ultrasound\"])\n",
    "y = results_df[\"cancertype\"]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), X.select_dtypes(include=['float64', 'int64']).columns),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), X.select_dtypes(include=['object']).columns)\n",
    "    ])\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca5678",
   "metadata": {},
   "source": [
    "#### train each model and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b7e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Finding best parameters for {name} with PCA...\")\n",
    "    \n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {}\n",
    "    if name == \"Random Forest\":\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100, 300, 500],\n",
    "            'classifier__max_features': ['auto', 'sqrt'],\n",
    "            'classifier__max_depth': [10, 50, 100, None],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif name == \"Support Vector Machine\":\n",
    "        param_grid = {\n",
    "            'classifier__C': [0.1, 1, 10, 100],\n",
    "            'classifier__gamma': [1, 0.1, 0.01, 0.001],\n",
    "            'classifier__kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    elif name == \"XGBoost\":\n",
    "        param_grid = {\n",
    "            'classifier__learning_rate': [0.1, 0.01, 0.05],\n",
    "            'classifier__max_depth': [3, 5, 7,],\n",
    "            'classifier__min_child_weight': [1, 3, 5],\n",
    "            'classifier__gamma': [0.1, 0.2, 0.3],\n",
    "            'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "            'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'classifier__objective': ['binary:logistic'],\n",
    "            'classifier__n_estimators': [100, 200, 300]\n",
    "        }\n",
    "    elif name == \"Gradient Boosting\":\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100, 300, 500],\n",
    "            'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'classifier__max_depth': [3, 5, 7],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__subsample': [0.8, 0.9, 1.0],\n",
    "            'classifier__max_features': ['auto', 'sqrt']\n",
    "        }\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=Pipeline(steps=[(\"preprocessor\", preprocessor), (\"pca\", PCA()), (\"classifier\", clf)]),\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters for {name} with PCA: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Training {name} with best parameters and PCA...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    dump(best_model, f\"models/{name}_model_with_PCA.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecae217",
   "metadata": {},
   "source": [
    "#### train other models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e032afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(estimator=DecisionTreeClassifier(), learning_rate=0.1), \n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Finding best parameters for {name}...\")\n",
    "    param_grid = {}\n",
    "    if name == \"Decision Tree\":\n",
    "        param_grid = {\n",
    "            'classifier__criterion': ['gini', 'entropy'], \n",
    "            'classifier__max_depth': [None, 10, 50, 100],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", clf)]),\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Training {name} with best parameters...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    dump(best_model, f\"models/{name}_model2.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23b8d1",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center; font-size:64px;\">Training Models with Selected Features</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a32d2",
   "metadata": {},
   "source": [
    "<p> In this part, we select features according to feature importance score and correlation map results.</p>\n",
    "<p> And then, we train the models with best parameters and save them.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bec319",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7f90f",
   "metadata": {},
   "source": [
    "#### load dataset and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"radiomic_train.csv\")\n",
    "\n",
    "X = results_df[[\"original_glcm_SumEntropy\",\n",
    "\"original_glcm_JointEnergy\",\n",
    "\"original_ngtdm_Complexity\",\n",
    "\"original_glcm_Correlation\",\n",
    "\"original_gldm_DependenceEntropy\",\n",
    "\"original_glcm_SumSquares\",\n",
    "\"original_firstorder_Uniformity\",\n",
    "\"original_ngtdm_Coarseness\",\n",
    "\"original_shape2D_PerimeterSurfaceRatio\",\n",
    "\"original_glszm_LowGrayLevelZoneEmphasis\",\n",
    "\"original_glszm_ZoneEntropy\",\n",
    "\"original_shape2D_PixelSurface\",\n",
    "\"original_glszm_SizeZoneNonUniformity\",\n",
    "\"original_shape2D_Perimeter\",\n",
    "\"original_gldm_DependenceNonUniformityNormalized\",\n",
    "\"original_glszm_SmallAreaLowGrayLevelEmphasis\",\n",
    "\"original_glrlm_GrayLevelNonUniformity\",\n",
    "\"original_glszm_SizeZoneNonUniformityNormalized\",\n",
    "\"original_shape2D_MajorAxisLength\",\n",
    "\"original_glcm_DifferenceVariance\",\n",
    "\"original_glcm_Idmn\",\n",
    "\"original_firstorder_Entropy\",\n",
    "\"original_shape2D_MaximumDiameter\",\n",
    "\"original_glcm_DifferenceAverage\",\n",
    "\"original_glcm_JointAverage\",\n",
    "\"original_glrlm_LowGrayLevelRunEmphasis\",\n",
    "\"original_glcm_Imc1\",\n",
    "\"original_glrlm_RunEntropy\",\n",
    "\"original_glcm_ClusterTendency\",\n",
    "\"original_glrlm_RunLengthNonUniformity\",\n",
    "\"original_glcm_MaximumProbability\",\n",
    "\"original_glcm_ClusterShade\",\n",
    "\"original_firstorder_Variance\",\n",
    "\"original_glrlm_GrayLevelVariance\",\n",
    "\"original_shape2D_PerimeterSurfaceRatio\",\n",
    "\"original_shape2D_Sphericity\",\n",
    "\"original_glcm_JointEntropy\",\n",
    "\"original_firstorder_RootMeanSquared\",\n",
    "\"original_glrlm_RunLengthNonUniformity\",\n",
    "\"original_gldm_HighGrayLevelEmphasis\",\n",
    "\"original_gldm_DependenceNonUniformity\",\n",
    "\"original_glcm_Idn\",\n",
    "\"original_ngtdm_Contrast\",\n",
    "\"original_shape2D_MeshSurface\",\n",
    "\"original_gldm_LargeDependenceLowGrayLevelEmphasis\",\n",
    "\"original_shape2D_Elongation\",\n",
    "\"original_glszm_SmallAreaHighGrayLevelEmphasis\",\n",
    "\"original_shape2D_MinorAxisLength\",\n",
    "\"original_gldm_LowGrayLevelEmphasis\",\n",
    "\"original_glcm_Contrast\",\n",
    "\"original_glszm_SmallAreaEmphasis\"]]\n",
    "y = results_df[\"cancertype\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), X.select_dtypes(include=['float64', 'int64']).columns),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), X.select_dtypes(include=['object']).columns)\n",
    "    ])\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b0145",
   "metadata": {},
   "source": [
    "#### train each model and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in classifiers.items():\n",
    "    print(f\"Finding best parameters for {name}...\")\n",
    "    param_grid = {}\n",
    "    if name == \"Random Forest\":\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100, 300, 500],\n",
    "            'classifier__max_features': ['auto', 'sqrt'],\n",
    "            'classifier__max_depth': [10, 50, 100, None],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif name == \"Support Vector Machine\":\n",
    "        param_grid = {\n",
    "            'classifier__C': [0.1, 1, 10, 100],\n",
    "            'classifier__gamma': [1, 0.1, 0.01, 0.001],\n",
    "            'classifier__kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    elif name == \"XGBoost\":\n",
    "        param_grid = {\n",
    "            'classifier__learning_rate': [0.1, 0.01, 0.05],\n",
    "            'classifier__max_depth': [3, 5, 7,],\n",
    "            'classifier__min_child_weight': [1, 3, 5],\n",
    "            'classifier__gamma': [0.1, 0.2, 0.3],\n",
    "            'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "            'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'classifier__objective': ['binary:logistic'],\n",
    "            'classifier__n_estimators': [100, 200, 300]\n",
    "        }\n",
    "    elif name == \"Gradient Boosting\":\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100, 300, 500],\n",
    "            'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'classifier__max_depth': [3, 5, 7],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__subsample': [0.8, 0.9, 1.0],\n",
    "            'classifier__max_features': ['auto', 'sqrt']\n",
    "        }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", clf)]),\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Training {name} with best parameters...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    dump(best_model, f\"models/{name}_selected2.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd3ea4",
   "metadata": {},
   "source": [
    "#### train other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(estimator=DecisionTreeClassifier(), learning_rate=0.1), \n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Finding best parameters for {name}...\")\n",
    "    param_grid = {}\n",
    "    if name == \"Decision Tree\":\n",
    "        param_grid = {\n",
    "            'classifier__criterion': ['gini', 'entropy'], \n",
    "            'classifier__max_depth': [None, 10, 50, 100],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", clf)]),\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Training {name} with best parameters...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    dump(best_model, f\"models/{name}_selected2.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec3d21",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center; font-size:64px;\">Classification Comparison</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12772f",
   "metadata": {},
   "source": [
    "<p>This part includes some classification calculations both all-features models and selected-features models.</p>\n",
    "<p>Also visualize them with matpllotlib.</p>\n",
    "<p>Support Vector Machine does not support probability predictions.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434e819",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, matthews_corrcoef, f1_score\n",
    "from joblib import load\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cd29b",
   "metadata": {},
   "source": [
    "#### calculation of classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0703bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"radiomic_features.csv\")\n",
    "\n",
    "y = results_df[\"cancertype\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "X_test = results_df.drop(columns=[\"cancertype\", \"ultrasound\"])\n",
    "y_test = results_df[\"cancertype\"]\n",
    "\n",
    "model_names = [\"Random Forest\", \"Support Vector Machine\", \"XGBoost\", \"Gradient Boosting\", \"AdaBoost\", \"Decision Tree\"]\n",
    "\n",
    "avg_accuracy = []\n",
    "avg_sensitivity = []\n",
    "avg_specificity = []\n",
    "avg_precision = []\n",
    "avg_recall = []\n",
    "avg_auc = []\n",
    "avg_mcc = []\n",
    "avg_fscore = []\n",
    "\n",
    "report_df = pd.DataFrame(columns=[\"Model\", \"Classification Report\"])\n",
    "\n",
    "for model_name in model_names:\n",
    "    model = load(f\"models/{model_name}_model.joblib\")\n",
    "    \n",
    "    y_pred_encoded = model.predict(X_test)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    avg_accuracy.append(accuracy)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.concat([report_df, pd.DataFrame({\"Model\": [model_name], \"Classification Report\": [report]})], ignore_index=True)\n",
    "\n",
    "    classes = list(report.keys())\n",
    "    classes = [cls for cls in classes if cls not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "    \n",
    "    recalls = [report[cls]['recall'] for cls in classes]\n",
    "    precisions = [report[cls]['precision'] for cls in classes]\n",
    "    avg_recall.append(np.mean(recalls))\n",
    "    avg_precision.append(np.mean(precisions))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    specificities = []\n",
    "    for i, cls in enumerate(classes):\n",
    "        tn = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i])\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        specificity = tn / (tn + fp)\n",
    "        specificities.append(specificity)\n",
    "    \n",
    "    avg_specificity.append(np.mean(specificities))\n",
    "    \n",
    "    if len(classes) == 2: \n",
    "        y_test_bin = label_encoder.transform(y_test)\n",
    "        y_pred_bin = label_encoder.transform(y_pred)\n",
    "        auc = roc_auc_score(y_test_bin, y_pred_bin)\n",
    "        avg_auc.append(auc)\n",
    "    else:  \n",
    "        auc = roc_auc_score(label_encoder.transform(y_test), model.predict_proba(X_test), multi_class='ovo')\n",
    "        avg_auc.append(auc)\n",
    "    \n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    fscore = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    avg_mcc.append(mcc)\n",
    "    avg_fscore.append(fscore)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Model\": model_names,\n",
    "    \"Accuracy\": avg_accuracy,\n",
    "    \"Sensitivity\": avg_recall,  \n",
    "    \"Specificity\": avg_specificity,\n",
    "    \"Precision\": avg_precision,\n",
    "    \"Recall\": avg_recall,\n",
    "    \"AUC\": avg_auc,\n",
    "    \"MCC\": avg_mcc,\n",
    "    \"F-score\": avg_fscore\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9e03d",
   "metadata": {},
   "source": [
    "#### calculation for selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2 = pd.read_csv(\"radiomic_features.csv\")\n",
    "\n",
    "y2 = results_df2[\"cancertype\"]\n",
    "label_encoder2 = LabelEncoder()\n",
    "label_encoder2.fit(y2)\n",
    "\n",
    "X_test2 = results_df2[[\"original_glcm_SumEntropy\",\"original_glcm_JointEnergy\",\"original_ngtdm_Complexity\",\n",
    "\"original_glcm_Correlation\",\"original_gldm_DependenceEntropy\",\"original_glcm_SumSquares\",\"original_firstorder_Uniformity\",\n",
    "\"original_ngtdm_Coarseness\",\"original_shape2D_PerimeterSurfaceRatio\",\"original_glszm_LowGrayLevelZoneEmphasis\",\n",
    "\"original_glszm_ZoneEntropy\",\"original_shape2D_PixelSurface\",\"original_glszm_SizeZoneNonUniformity\",\n",
    "\"original_shape2D_Perimeter\",\"original_gldm_DependenceNonUniformityNormalized\",\"original_glszm_SmallAreaLowGrayLevelEmphasis\",\n",
    "\"original_glrlm_GrayLevelNonUniformity\",\"original_glszm_SizeZoneNonUniformityNormalized\",\"original_shape2D_MajorAxisLength\",\n",
    "\"original_glcm_DifferenceVariance\",\"original_glcm_Idmn\",\"original_firstorder_Entropy\",\"original_shape2D_MaximumDiameter\",\n",
    "\"original_glcm_DifferenceAverage\",\"original_glcm_JointAverage\",\"original_glrlm_LowGrayLevelRunEmphasis\",\"original_glcm_Imc1\",\n",
    "\"original_glrlm_RunEntropy\",\"original_glcm_ClusterTendency\",\n",
    "\"original_glcm_MaximumProbability\",\"original_glcm_ClusterShade\",\"original_firstorder_Variance\",\"original_glrlm_GrayLevelVariance\",\"original_shape2D_Sphericity\",\"original_glcm_JointEntropy\",\n",
    "\"original_firstorder_RootMeanSquared\",\"original_glrlm_RunLengthNonUniformity\",\"original_gldm_HighGrayLevelEmphasis\",\n",
    "\"original_gldm_DependenceNonUniformity\",\"original_glcm_Idn\",\"original_ngtdm_Contrast\",\"original_shape2D_MeshSurface\",\n",
    "\"original_gldm_LargeDependenceLowGrayLevelEmphasis\",\"original_glszm_SmallAreaHighGrayLevelEmphasis\",\n",
    "\"original_shape2D_MinorAxisLength\",\"original_shape2D_Elongation\",\"original_gldm_LowGrayLevelEmphasis\",\n",
    "\"original_glcm_Contrast\",\"original_glszm_SmallAreaEmphasis\"]]\n",
    "y_test2 = results_df2[\"cancertype\"]\n",
    "\n",
    "model_names2 = [\"Random Forest\", \"Support Vector Machine\", \"XGBoost\", \"Gradient Boosting\", \"AdaBoost\", \"Decision Tree\"]\n",
    "\n",
    "avg_accuracy2 = []\n",
    "avg_sensitivity2 = []\n",
    "avg_specificity2 = []\n",
    "avg_precision2 = []\n",
    "avg_recall2 = []\n",
    "avg_auc2 = []\n",
    "avg_mcc2 = []\n",
    "avg_fscore2 = []\n",
    "\n",
    "report_df2 = pd.DataFrame(columns=[\"Model\", \"Classification Report\"])\n",
    "\n",
    "for model_name2 in model_names2:\n",
    "    model2 = load(f\"models/{model_name2}_selected2.joblib\")\n",
    "    \n",
    "    y_pred_encoded2 = model2.predict(X_test2)\n",
    "    y_pred2 = label_encoder2.inverse_transform(y_pred_encoded2)\n",
    "    \n",
    "    accuracy2 = accuracy_score(y_test2, y_pred2)\n",
    "    avg_accuracy2.append(accuracy2)\n",
    "    \n",
    "    report2 = classification_report(y_test2, y_pred2, output_dict=True)\n",
    "    report_df2 = pd.concat([report_df2, pd.DataFrame({\"Model\": [model_name2], \"Classification Report\": [report2]})], ignore_index=True)\n",
    "\n",
    "    classes2 = list(report2.keys())\n",
    "    classes2 = [cls2 for cls2 in classes2 if cls2 not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "    \n",
    "    recalls2 = [report2[cls2]['recall'] for cls2 in classes2]\n",
    "    precisions2 = [report2[cls2]['precision'] for cls2 in classes2]\n",
    "    avg_recall2.append(np.mean(recalls2))\n",
    "    avg_precision2.append(np.mean(precisions2))\n",
    "    \n",
    "    cm2 = confusion_matrix(y_test2, y_pred2)\n",
    "    specificities2 = []\n",
    "    for i2, cls2 in enumerate(classes2):\n",
    "        tn2 = np.sum(cm2) - (np.sum(cm2[i2, :]) + np.sum(cm2[:, i2]) - cm2[i2, i2])\n",
    "        fp2 = np.sum(cm2[:, i2]) - cm2[i2, i2]\n",
    "        specificity2 = tn2 / (tn2 + fp2)\n",
    "        specificities2.append(specificity2)\n",
    "    \n",
    "    avg_specificity2.append(np.mean(specificities2))\n",
    "    \n",
    "    if len(classes2) == 2: \n",
    "        y_test_bin2 = label_encoder2.transform(y_test2)\n",
    "        y_pred_bin2 = label_encoder2.transform(y_pred2)\n",
    "        auc2 = roc_auc_score(y_test_bin2, y_pred_bin2)\n",
    "        avg_auc2.append(auc2)\n",
    "    else:  \n",
    "        auc2 = roc_auc_score(label_encoder2.transform(y_test2), model2.predict_proba(X_test2), multi_class='ovo')\n",
    "        avg_auc2.append(auc2)\n",
    "    \n",
    "    mcc2 = matthews_corrcoef(y_test2, y_pred2)\n",
    "    fscore2 = f1_score(y_test2, y_pred2, average='weighted')\n",
    "    \n",
    "    avg_mcc2.append(mcc2)\n",
    "    avg_fscore2.append(fscore2)\n",
    "\n",
    "metrics_df2 = pd.DataFrame({\n",
    "    \"Model\": model_names2,\n",
    "    \"Accuracy\": avg_accuracy2,\n",
    "    \"Sensitivity\": avg_recall2,  \n",
    "    \"Specificity\": avg_specificity2,\n",
    "    \"Precision\": avg_precision2,\n",
    "    \"Recall\": avg_recall2,\n",
    "    \"AUC\": avg_auc2,\n",
    "    \"MCC\": avg_mcc2,\n",
    "    \"F-score\": avg_fscore2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_excel(\"classification_reports_allfeatures.xlsx\", index=False)\n",
    "report_df2.to_excel(\"classification_reports_selected.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afc60a",
   "metadata": {},
   "source": [
    "#### graph 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(model_names)) \n",
    "width = 0.25 \n",
    "\n",
    "rects1 = ax1.bar(x - width, metrics_df[\"Accuracy\"], width, label='Accuracy')\n",
    "rects2 = ax1.bar(x, metrics_df[\"Sensitivity\"], width, label='Sensitivity')\n",
    "rects3 = ax1.bar(x + width, metrics_df[\"Specificity\"], width, label='Specificity')\n",
    "\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('Scores')\n",
    "ax1.set_title('Comparison of Accuracy, Sensitivity, and Specificity')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(model_names)\n",
    "ax1.legend()\n",
    "\n",
    "def add_value_labels(rects, ax):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(round(height, 2)),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  \n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_value_labels(rects1, ax1)\n",
    "add_value_labels(rects2, ax1)\n",
    "add_value_labels(rects3, ax1)\n",
    "\n",
    "fig1.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig1_2, ax1_2 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x2 = np.arange(len(model_names2)) \n",
    "width2 = 0.25 \n",
    "\n",
    "rects1_2 = ax1_2.bar(x2 - width2, metrics_df2[\"Accuracy\"], width2, label='Accuracy')\n",
    "rects2_2 = ax1_2.bar(x2, metrics_df2[\"Sensitivity\"], width2, label='Sensitivity')\n",
    "rects3_2 = ax1_2.bar(x2 + width2, metrics_df2[\"Specificity\"], width2, label='Specificity')\n",
    "\n",
    "ax1_2.set_xlabel('Models with Selected Features')\n",
    "ax1_2.set_ylabel('Scores')\n",
    "ax1_2.set_title('Comparison of Accuracy, Sensitivity, and Specificity')\n",
    "ax1_2.set_xticks(x2)\n",
    "ax1_2.set_xticklabels(model_names2)\n",
    "ax1_2.legend()\n",
    "\n",
    "def add_value_labels2(rects2, ax2):\n",
    "    for rect2 in rects2:\n",
    "        height2 = rect2.get_height()\n",
    "        ax2.annotate('{}'.format(round(height2, 2)),\n",
    "                    xy=(rect2.get_x() + rect2.get_width() / 2, height2),\n",
    "                    xytext=(0, 3),  \n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_value_labels2(rects1_2, ax1_2)\n",
    "add_value_labels2(rects2_2, ax1_2)\n",
    "add_value_labels2(rects3_2, ax1_2)\n",
    "\n",
    "fig1_2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f22f00",
   "metadata": {},
   "source": [
    "#### graph 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6aabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "rects4 = ax2.bar(x - width/2, metrics_df[\"Precision\"], width, label='Precision')\n",
    "rects5 = ax2.bar(x + width/2, metrics_df[\"Recall\"], width, label='Recall')\n",
    "\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.set_ylabel('Scores')\n",
    "ax2.set_title('Comparison of Precision and Recall')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(model_names)\n",
    "ax2.legend()\n",
    "\n",
    "add_value_labels(rects4, ax2)\n",
    "add_value_labels(rects5, ax2)\n",
    "\n",
    "fig2.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig2_2, ax2_2 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "rects4_2 = ax2_2.bar(x2 - width2/2, metrics_df2[\"Precision\"], width2, label='Precision')\n",
    "rects5_2 = ax2_2.bar(x2 + width2/2, metrics_df2[\"Recall\"], width2, label='Recall')\n",
    "\n",
    "ax2_2.set_xlabel('Models with Selected Features')\n",
    "ax2_2.set_ylabel('Scores')\n",
    "ax2_2.set_title('Comparison of Precision and Recall')\n",
    "ax2_2.set_xticks(x2)\n",
    "ax2_2.set_xticklabels(model_names2)\n",
    "ax2_2.legend()\n",
    "\n",
    "add_value_labels2(rects4_2, ax2_2)\n",
    "add_value_labels2(rects5_2, ax2_2)\n",
    "\n",
    "fig2_2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad6e9a",
   "metadata": {},
   "source": [
    "#### graph 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "rects6 = ax3.bar(x - width, metrics_df[\"AUC\"], width, label='AUC')\n",
    "rects7 = ax3.bar(x, metrics_df[\"MCC\"], width, label='MCC')\n",
    "rects8 = ax3.bar(x + width, metrics_df[\"F-score\"], width, label='F-score')\n",
    "\n",
    "ax3.set_xlabel('Model')\n",
    "ax3.set_ylabel('Scores')\n",
    "ax3.set_title('Comparison of AUC, MCC, and F-score')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(model_names)\n",
    "ax3.legend()\n",
    "\n",
    "add_value_labels(rects6, ax3)\n",
    "add_value_labels(rects7, ax3)\n",
    "add_value_labels(rects8, ax3)\n",
    "\n",
    "fig3.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig3_2, ax3_2 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "rects6_2 = ax3_2.bar(x2 - width2, metrics_df2[\"AUC\"], width2, label='AUC')\n",
    "rects7_2 = ax3_2.bar(x2, metrics_df2[\"MCC\"], width2, label='MCC')\n",
    "rects8_2 = ax3_2.bar(x2 + width2, metrics_df2[\"F-score\"], width2, label='F-score')\n",
    "\n",
    "ax3_2.set_xlabel('Models with Selected Features')\n",
    "ax3_2.set_ylabel('Scores')\n",
    "ax3_2.set_title('Comparison of AUC, MCC, and F-score')\n",
    "ax3_2.set_xticks(x2)\n",
    "ax3_2.set_xticklabels(model_names2)\n",
    "ax3_2.legend()\n",
    "\n",
    "add_value_labels2(rects6_2, ax3_2)\n",
    "add_value_labels2(rects7_2, ax3_2)\n",
    "add_value_labels2(rects8_2, ax3_2)\n",
    "\n",
    "fig3_2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90f68d",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center; font-size:64px;\">Roc Curve and Confusion Matrixes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a442b17",
   "metadata": {},
   "source": [
    "<p>In this part, we calculate Receiver Operating Characteristic (ROC) Curve and Confusion Matrixes both all-features model and selected-features models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5482de",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ec69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from joblib import load\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c22c0",
   "metadata": {},
   "source": [
    "#### load dataset and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91253a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"radiomic_test.csv\")\n",
    "\n",
    "y = results_df[\"cancertype\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "X_test = results_df.drop(columns=[\"cancertype\", \"ultrasound\"])\n",
    "y_test = results_df[\"cancertype\"]\n",
    "\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "model_names = [\"Random Forest\", \"Support Vector Machine\", \"XGBoost\", \"Gradient Boosting\", \"AdaBoost\", \"Decision Tree\"]\n",
    "\n",
    "\n",
    "results_df2 = pd.read_csv(\"radiomic_features.csv\")\n",
    "\n",
    "y2 = results_df2[\"cancertype\"]\n",
    "label_encoder2 = LabelEncoder()\n",
    "label_encoder2.fit(y2)\n",
    "\n",
    "X_test2 = results_df2[[\"original_glcm_SumEntropy\",\n",
    "\"original_glcm_JointEnergy\",\n",
    "\"original_ngtdm_Complexity\",\n",
    "\"original_glcm_Correlation\",\n",
    "\"original_gldm_DependenceEntropy\",\n",
    "\"original_glcm_SumSquares\",\n",
    "\"original_firstorder_Uniformity\",\n",
    "\"original_ngtdm_Coarseness\",\n",
    "\"original_shape2D_PerimeterSurfaceRatio\",\n",
    "\"original_glszm_LowGrayLevelZoneEmphasis\",\n",
    "\"original_glszm_ZoneEntropy\",\n",
    "\"original_shape2D_PixelSurface\",\n",
    "\"original_glszm_SizeZoneNonUniformity\",\n",
    "\"original_shape2D_Perimeter\",\n",
    "\"original_gldm_DependenceNonUniformityNormalized\",\n",
    "\"original_glszm_SmallAreaLowGrayLevelEmphasis\",\n",
    "\"original_glrlm_GrayLevelNonUniformity\",\n",
    "\"original_glszm_SizeZoneNonUniformityNormalized\",\n",
    "\"original_shape2D_MajorAxisLength\",\n",
    "\"original_glcm_DifferenceVariance\",\n",
    "\"original_glcm_Idmn\",\n",
    "\"original_firstorder_Entropy\",\n",
    "\"original_shape2D_MaximumDiameter\",\n",
    "\"original_glcm_DifferenceAverage\",\n",
    "\"original_glcm_JointAverage\",\n",
    "\"original_glrlm_LowGrayLevelRunEmphasis\",\n",
    "\"original_glcm_Imc1\",\n",
    "\"original_glrlm_RunEntropy\",\n",
    "\"original_glcm_ClusterTendency\",\n",
    "\"original_glrlm_RunLengthNonUniformity\",\n",
    "\"original_glcm_MaximumProbability\",\n",
    "\"original_glcm_ClusterShade\",\n",
    "\"original_firstorder_Variance\",\n",
    "\"original_glrlm_GrayLevelVariance\",\n",
    "\"original_shape2D_PerimeterSurfaceRatio\",\n",
    "\"original_shape2D_Sphericity\",\n",
    "\"original_glcm_JointEntropy\",\n",
    "\"original_firstorder_RootMeanSquared\",\n",
    "\"original_glrlm_RunLengthNonUniformity\",\n",
    "\"original_gldm_HighGrayLevelEmphasis\",\n",
    "\"original_gldm_DependenceNonUniformity\",\n",
    "\"original_glcm_Idn\",\n",
    "\"original_ngtdm_Contrast\",\n",
    "\"original_shape2D_MeshSurface\",\n",
    "\"original_gldm_LargeDependenceLowGrayLevelEmphasis\",\n",
    "\"original_shape2D_Elongation\",\n",
    "\"original_glszm_SmallAreaHighGrayLevelEmphasis\",\n",
    "\"original_shape2D_MinorAxisLength\",\n",
    "\"original_gldm_LowGrayLevelEmphasis\",\n",
    "\"original_glcm_Contrast\",\n",
    "\"original_glszm_SmallAreaEmphasis\"]]\n",
    "y_test2 = results_df2[\"cancertype\"]\n",
    "\n",
    "y_test_encoded2 = label_encoder2.transform(y_test2)\n",
    "\n",
    "model_names2 = [\"Random Forest\", \"Support Vector Machine\", \"XGBoost\", \"Gradient Boosting\", \"AdaBoost\", \"Decision Tree\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a858a37",
   "metadata": {},
   "source": [
    "#### roc curve graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['malignant', 'benign']  \n",
    "\n",
    "for model_name in model_names:\n",
    "    model = load(f\"models/{model_name}_model.joblib\")\n",
    "    \n",
    "    try:\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "    except AttributeError:\n",
    "        print(f\"{model_name} does not support probability predictions.\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        fpr, tpr, _ = roc_curve(y_test_encoded == i, y_pred_proba[:, i])\n",
    "        roc_auc = roc_auc_score(y_test_encoded == i, y_pred_proba[:, i])\n",
    "        \n",
    "        ax.plot(fpr, tpr, lw=2,\n",
    "                label=f'{class_name} (area = {roc_auc:.2f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'Receiver Operating Characteristic (ROC) Curve for {model_name}')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "for model_name2 in model_names2:\n",
    "    model2 = load(f\"models/{model_name2}_selected.joblib\")\n",
    "    \n",
    "    try:\n",
    "        y_pred_proba2 = model2.predict_proba(X_test2)\n",
    "    except AttributeError:\n",
    "        print(f\"{model_name2} does not support probability predictions.\")\n",
    "        continue\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        fpr2, tpr2, _2 = roc_curve(y_test_encoded2 == i, y_pred_proba2[:, i])\n",
    "        roc_auc2 = roc_auc_score(y_test_encoded2 == i, y_pred_proba2[:, i])\n",
    "        \n",
    "        ax2.plot(fpr2, tpr2, lw=2,\n",
    "                 label=f'{class_name} (area = {roc_auc2:.2f})')\n",
    "    \n",
    "    ax2.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title(f'Receiver Operating Characteristic (ROC) Curve for {model_name2} with Selected Features')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763dc6b",
   "metadata": {},
   "source": [
    "#### confusion matrix graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "fig.suptitle('CONFUSION MATRIXES', fontsize=16)  \n",
    "\n",
    "for ax, model_name in zip(axes.flat, model_names):\n",
    "    model = load(f\"models/{model_name}_model.joblib\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test_encoded, y_pred)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "    disp.plot(ax=ax, cmap='Blues')\n",
    "    ax.set_title(f'{model_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig2, axes2 = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "fig2.suptitle('CONFUSION MATRIXES FOR SELECTED FEATURES', fontsize=16)  \n",
    "\n",
    "for ax2, model_name2 in zip(axes2.flat, model_names2):\n",
    "    model2 = load(f\"models/{model_name2}_selected.joblib\")\n",
    "    \n",
    "    y_pred2 = model2.predict(X_test2)\n",
    "    \n",
    "    cm2 = confusion_matrix(y_test_encoded2, y_pred2)\n",
    "    \n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=label_encoder2.classes_)\n",
    "    disp2.plot(ax=ax2, cmap='Blues')\n",
    "    ax2.set_title(f'{model_name2}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
